{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yy3462-create/moma_artist_analysis/blob/main/youtube_vader_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKQd7o32-xdS"
      },
      "source": [
        "# YouTube + VADER Sentiment â€” Student Guide & Live Coding Notebook\n",
        "\n",
        "> Run this end-to-end in Google Colab. Cells are heavily commented so you can follow what each line does."
      ],
      "id": "DKQd7o32-xdS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn9LcC6B-xdW"
      },
      "source": [
        "## Part 0 â€” VADER Warmâ€‘Up (Strengths & Limitations)\n",
        "Weâ€™ll start with a small â€œsentiment sandboxâ€ to see where VADER shines (short, social text; emojis/emphasis) and where it struggles (sarcasm, domain slang, negation edge cases).\n",
        "\n",
        "### What does the score represent?\n",
        "VADERâ€™s compound score = lexicon scores + rule-based adjustments â†’ normalized into [â€“1, 1].\n",
        "\n",
        "It captures sentiment in a way thatâ€™s simple, interpretable, powerful, and built for internet languageâ€”which makes it a popular choice for policy, media, and social analysis."
      ],
      "id": "mn9LcC6B-xdW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW30WvhS-xdW"
      },
      "source": [
        "### 0.1 Install & Imports"
      ],
      "id": "LW30WvhS-xdW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iWPlOjQ-xdW",
        "outputId": "fb28760b-c97b-4b77-8253-d5e78d40d182",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.9/1.5 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Keep pandas (already in Colab). Ensure latest NLTK.\n",
        "!pip -q install --upgrade nltk\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download the VADER lexicon (only needs to run once per runtime)\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n"
      ],
      "id": "3iWPlOjQ-xdW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsxb56m2-xdX"
      },
      "source": [
        "### 0.2 Quick word tests"
      ],
      "id": "Fsxb56m2-xdX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp_8iJYH-xdX"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# Try a variety of words and short phrases.\n",
        "# Notice how intensifiers, punctuation, and emojis change scores.\n",
        "examples = [\n",
        "    \"good\", \"great!!!\", \"bad\", \"terrible\", \"okay\", \"not good\", \"not bad\",\n",
        "    \"sick\", \"sick!\", \"sick ðŸ¤®\", \"sick ðŸ¤©\",  # same token, different meaning via emoji\n",
        "    \"love\", \"hate\", \"meh\", \"LOL\", \"lol\", \"LOL!!!\",  # casing & emphasis\n",
        "]\n",
        "\n",
        "results = []\n",
        "for s in examples:\n",
        "    scores = sia.polarity_scores(s)\n",
        "    results.append({\"text\": s, **scores})\n",
        "\n",
        "pd.DataFrame(results).sort_values(\"compound\", ascending=False)\n"
      ],
      "id": "xp_8iJYH-xdX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA0AOyXM-xdX"
      },
      "source": [
        "### 0.3 Sentenceâ€‘level experiments"
      ],
      "id": "xA0AOyXM-xdX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdnuol8d-xdY"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# Explore common edge cases: negation, contrastive conjunctions, sarcasm, domain slang.\n",
        "edge_cases = [\n",
        "    \"This was good, not great.\",\n",
        "    \"This was not good.\",\n",
        "    \"This was good but also kind of annoying.\",\n",
        "    \"Yeah right, amazing...\",  # sarcasm\n",
        "    \"The movie was fire\",      # slang (positive in many contexts)\n",
        "    \"The service was mad slow\",# regional slang (negative)\n",
        "    \"I love how it crashes every five minutes\", # sarcasm\n",
        "]\n",
        "\n",
        "pd.DataFrame([{ \"text\": s, **sia.polarity_scores(s)} for s in edge_cases])\n"
      ],
      "id": "Cdnuol8d-xdY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x42dYw5y-xdY"
      },
      "source": [
        "**Things to note:**\n",
        "- VADER is a **lexicon + rules** approach; it leverages booster words (e.g., *very*), punctuation, capitalization, emojis.\n",
        "- It can miss **sarcasm** or domain meanings (e.g., *sick* can be good or bad).\n",
        "- For short, socialâ€‘style comments VADER performs surprisingly wellâ€”but you should always sanityâ€‘check results.\n",
        "\n",
        "---"
      ],
      "id": "x42dYw5y-xdY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqqWn1Co-xdY"
      },
      "source": [
        "## Part 1 â€” Your YouTube API Key\n",
        "Youâ€™ll need a Google Cloud project with **YouTube Data API v3** enabled. Keep your key private.\n",
        "\n",
        "**Steps**\n",
        "1. Go to **Google Cloud Console** â†’ create (or select) a **Project**.\n",
        "2. **APIs & Services** â†’ **Enable APIs and Services** â†’ search **YouTube Data API v3** â†’ **Enable**.\n",
        "3. **Credentials** â†’ **Create Credentials** â†’ **API key**.\n",
        "4. (Optional but recommended) **Restrict** the key (HTTP referrers or IP).\n",
        "5. Paste it into the Colab runtime via a hidden prompt:"
      ],
      "id": "SqqWn1Co-xdY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg3GQ_iT-xdY"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Paste your API key when prompted (input will be hidden in Colab)\n",
        "os.environ[\"YOUTUBE_API_KEY\"] = getpass(\"Paste your API Key: \")\n",
        "\n",
        "# Quick sanity check\n",
        "assert os.environ.get(\"YOUTUBE_API_KEY\"), \"API key not set â€” please run the cell and paste your key.\"\n"
      ],
      "id": "Dg3GQ_iT-xdY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfJteOyL-xdY"
      },
      "source": [
        "## Part 2 â€” Collect YouTube Data (Search â†’ Video Details â†’ Comments)\n",
        "Weâ€™ll use plain `requests` so you can see raw REST calls, params, and pagination. Then weâ€™ll tidy the results with `pandas`."
      ],
      "id": "MfJteOyL-xdY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk3QsVWs-xdZ"
      },
      "source": [
        "### 2.1 Install (if needed) & imports"
      ],
      "id": "mk3QsVWs-xdZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vajTp6nf-xdZ"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "!pip -q install requests tqdm\n",
        "\n",
        "import os\n",
        "import json\n",
        "from urllib.parse import urlencode\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n"
      ],
      "id": "vajTp6nf-xdZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIaM_z1C-xdZ"
      },
      "source": [
        "### 2.2 Helper: call the YouTube Data API"
      ],
      "id": "dIaM_z1C-xdZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tuk7fvqB-xdZ"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "API_KEY = os.environ.get(\"YOUTUBE_API_KEY\")\n",
        "BASE_URL = \"https://www.googleapis.com/youtube/v3\"\n",
        "\n",
        "if not API_KEY:\n",
        "    raise ValueError(\"Missing API key. Set os.environ['YOUTUBE_API_KEY'] first.\")\n",
        "\n",
        "def yt_get(resource: str, params: dict) -> dict:\n",
        "    \"\"\"Call YouTube Data API v3.\n",
        "    - resource: e.g., 'search', 'videos', 'commentThreads'\n",
        "    - params: dict of query params (we append the API key here)\n",
        "    Returns parsed JSON as a Python dict.\n",
        "    \"\"\"\n",
        "    q = {**params, \"key\": API_KEY}\n",
        "    url = f\"{BASE_URL}/{resource}?{urlencode(q)}\"\n",
        "    r = requests.get(url, timeout=30)\n",
        "    r.raise_for_status()  # raise an HTTPError if the request failed\n",
        "    return r.json()\n"
      ],
      "id": "Tuk7fvqB-xdZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxUOqDw_-xdZ"
      },
      "source": [
        "### 2.3 Search videos for a topic (collect video IDs)"
      ],
      "id": "SxUOqDw_-xdZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL9i1o-y-xdZ"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# âœ… Edit this query to explore your own topic\n",
        "QUERY = \"subway safety NYC\"  # e.g., \"climate policy\", \"Taylor Swift\"\n",
        "TARGET_VIDEOS = 60           # upper bound of total videos to collect (keep modest: quotas!)\n",
        "MAX_RESULTS = 50             # per-page limit for search endpoint\n",
        "\n",
        "video_hits = []              # will hold basic search results\n",
        "page_token = None            # used for pagination\n",
        "\n",
        "with tqdm(total=TARGET_VIDEOS, desc=\"Searching videos\") as pbar:\n",
        "    while len(video_hits) < TARGET_VIDEOS:\n",
        "        # The 'search' resource finds videos; we request snippet data (title, channel, publishedAt).\n",
        "        params = {\n",
        "            \"part\": \"snippet\",\n",
        "            \"q\": QUERY,\n",
        "            \"type\": \"video\",\n",
        "            \"maxResults\": MAX_RESULTS,\n",
        "            \"order\": \"relevance\",\n",
        "        }\n",
        "        if page_token:\n",
        "            params[\"pageToken\"] = page_token\n",
        "\n",
        "        data = yt_get(\"search\", params)\n",
        "        items = data.get(\"items\", [])\n",
        "\n",
        "        for it in items:\n",
        "            vid = it.get(\"id\", {}).get(\"videoId\")\n",
        "            if not vid:\n",
        "                continue\n",
        "            snip = it.get(\"snippet\", {})\n",
        "            video_hits.append({\n",
        "                \"video_id\": vid,\n",
        "                \"publishedAt\": snip.get(\"publishedAt\"),\n",
        "                \"title\": snip.get(\"title\"),\n",
        "                \"channelId\": snip.get(\"channelId\"),\n",
        "                \"channelTitle\": snip.get(\"channelTitle\"),\n",
        "            })\n",
        "            pbar.update(1)\n",
        "            if len(video_hits) >= TARGET_VIDEOS:\n",
        "                break\n",
        "\n",
        "        page_token = data.get(\"nextPageToken\")\n",
        "        if not page_token:\n",
        "            break  # no more pages\n",
        "\n",
        "videos_df = pd.DataFrame(video_hits)\n",
        "videos_df.head(3)\n"
      ],
      "id": "fL9i1o-y-xdZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw-dkbM--xdZ"
      },
      "source": [
        "### 2.4 Enrich videos: titles, descriptions, and stats"
      ],
      "id": "Fw-dkbM--xdZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c3PwPI1-xdZ"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# We'll call 'videos.list' to fetch details for batches of IDs (up to 50 per call)\n",
        "def chunked(seq, size):\n",
        "    for i in range(0, len(seq), size):\n",
        "        yield seq[i:i+size]\n",
        "\n",
        "video_ids = videos_df[\"video_id\"].dropna().unique().tolist()\n",
        "\n",
        "video_details = []\n",
        "for batch in tqdm(list(chunked(video_ids, 50)), desc=\"Fetching video details\"):\n",
        "    params = {\n",
        "        \"part\": \"snippet,statistics\",\n",
        "        \"id\": \",\".join(batch),\n",
        "        \"maxResults\": 50,\n",
        "    }\n",
        "    data = yt_get(\"videos\", params)\n",
        "    for it in data.get(\"items\", []):\n",
        "        snip = it.get(\"snippet\", {})\n",
        "        stats = it.get(\"statistics\", {})\n",
        "        video_details.append({\n",
        "            \"video_id\": it.get(\"id\"),\n",
        "            \"title\": snip.get(\"title\"),\n",
        "            \"description\": snip.get(\"description\"),\n",
        "            \"publishedAt\": snip.get(\"publishedAt\"),\n",
        "            \"channelTitle\": snip.get(\"channelTitle\"),\n",
        "            # Cast numeric strings to integers when possible\n",
        "            \"viewCount\": int(stats.get(\"viewCount\", 0) or 0),\n",
        "            \"likeCount\": int(stats.get(\"likeCount\", 0) or 0),\n",
        "            \"commentCount\": int(stats.get(\"commentCount\", 0) or 0),\n",
        "        })\n",
        "\n",
        "video_details_df = pd.DataFrame(video_details)\n",
        "video_details_df.head(3)\n"
      ],
      "id": "4c3PwPI1-xdZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPxO54sP-xdZ"
      },
      "source": [
        "### 2.5 Fetch topâ€‘level comments for each video (with pagination)"
      ],
      "id": "XPxO54sP-xdZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcBH05s1-xdZ"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# Some videos disable comments. We'll handle HTTP errors gracefully and cap perâ€‘video volume.\n",
        "all_comments = []\n",
        "\n",
        "for vid in tqdm(video_details_df[\"video_id\"].tolist(), desc=\"Fetching comments\"):\n",
        "    page_token = None\n",
        "    fetched = 0\n",
        "    try:\n",
        "        while True:\n",
        "            params = {\n",
        "                \"part\": \"snippet\",\n",
        "                \"videoId\": vid,\n",
        "                \"maxResults\": 100,  # API max per page for commentThreads\n",
        "                \"order\": \"relevance\",  # try 'time' if you want chronological\n",
        "                # 'textFormat': 'plainText' is default\n",
        "            }\n",
        "            if page_token:\n",
        "                params[\"pageToken\"] = page_token\n",
        "\n",
        "            data = yt_get(\"commentThreads\", params)\n",
        "            items = data.get(\"items\", [])\n",
        "\n",
        "            for it in items:\n",
        "                top = it.get(\"snippet\", {}).get(\"topLevelComment\", {})\n",
        "                s = top.get(\"snippet\", {})\n",
        "                all_comments.append({\n",
        "                    \"video_id\": vid,\n",
        "                    \"comment_id\": top.get(\"id\"),\n",
        "                    \"author\": s.get(\"authorDisplayName\"),\n",
        "                    \"publishedAt\": s.get(\"publishedAt\"),\n",
        "                    \"likeCount\": s.get(\"likeCount\", 0),\n",
        "                    \"text\": s.get(\"textOriginal\", \"\"),\n",
        "                })\n",
        "                fetched += 1\n",
        "\n",
        "            page_token = data.get(\"nextPageToken\")\n",
        "            if not page_token:\n",
        "                break  # no more pages\n",
        "\n",
        "            if fetched >= 300:\n",
        "                break  # safety cap so a single video doesnâ€™t eat your quota\n",
        "\n",
        "    except requests.HTTPError as e:\n",
        "        print(f\"Skipping {vid} due to HTTP error: {e}\")\n",
        "        continue\n",
        "\n",
        "comments_df = pd.DataFrame(all_comments)\n",
        "comments_df.head(3)\n"
      ],
      "id": "PcBH05s1-xdZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHWftzIk-xda"
      },
      "source": [
        "## Part 3 â€” Sentiment Analysis with VADER\n",
        "We will score titles, descriptions, and comments. Then weâ€™ll aggregate by video."
      ],
      "id": "vHWftzIk-xda"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgNP0Nzw-xda"
      },
      "source": [
        "### 3.1 Set up VADER (if you skipped Part 0)"
      ],
      "id": "tgNP0Nzw-xda"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQFvoJWX-xda"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# Already upgraded earlier; safe to reâ€‘run if needed\n",
        "!pip -q install --upgrade nltk\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n"
      ],
      "id": "CQFvoJWX-xda"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_F0bgLI-xda"
      },
      "source": [
        "### 3.2 Score text fields"
      ],
      "id": "c_F0bgLI-xda"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyIWw5YG-xda"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# Helper to score a text string and return only the 'compound' score ([-1, 1])\n",
        "def compound_score(text):\n",
        "    return sia.polarity_scores(text or \"\")[\"compound\"]\n",
        "\n",
        "# Video titles & descriptions\n",
        "video_details_df[\"title_compound\"] = video_details_df[\"title\"].fillna(\"\").apply(compound_score)\n",
        "video_details_df[\"description_compound\"] = video_details_df[\"description\"].fillna(\"\").apply(compound_score)\n",
        "\n",
        "# Comments (if any)\n",
        "if not comments_df.empty:\n",
        "    comments_df[\"compound\"] = comments_df[\"text\"].fillna(\"\").apply(compound_score)\n"
      ],
      "id": "GyIWw5YG-xda"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUZOx5Kg-xda"
      },
      "source": [
        "### 3.3 Aggregate to video level"
      ],
      "id": "tUZOx5Kg-xda"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHmOit66-xda"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# Common VADER thresholds\n",
        "POS, NEG = 0.05, -0.05\n",
        "\n",
        "if not comments_df.empty:\n",
        "    comments_df[\"sentiment_label\"] = comments_df[\"compound\"].apply(\n",
        "        lambda c: \"pos\" if c > POS else (\"neg\" if c < NEG else \"neu\")\n",
        "    )\n",
        "\n",
        "    agg = (comments_df.groupby(\"video_id\").agg(\n",
        "        n_comments=(\"comment_id\", \"count\"),\n",
        "        mean_compound=(\"compound\", \"mean\"),\n",
        "        pct_pos=(\"sentiment_label\", lambda s: (s == \"pos\").mean()),\n",
        "        pct_neg=(\"sentiment_label\", lambda s: (s == \"neg\").mean()),\n",
        "        pct_neu=(\"sentiment_label\", lambda s: (s == \"neu\").mean()),\n",
        "    ).reset_index())\n",
        "else:\n",
        "    # Empty placeholder so the merge below still works\n",
        "    agg = pd.DataFrame(columns=[\"video_id\", \"n_comments\", \"mean_compound\", \"pct_pos\", \"pct_neg\", \"pct_neu\"])\n",
        "\n",
        "summary = (\n",
        "    video_details_df.merge(agg, on=\"video_id\", how=\"left\")\n",
        "    .assign(\n",
        "        title_compound=lambda d: d[\"title_compound\"].round(3),\n",
        "        description_compound=lambda d: d[\"description_compound\"].round(3),\n",
        "        mean_compound=lambda d: d[\"mean_compound\"].round(3),\n",
        "        pct_pos=lambda d: (d[\"pct_pos\"]*100).round(1),\n",
        "        pct_neg=lambda d: (d[\"pct_neg\"]*100).round(1),\n",
        "        pct_neu=lambda d: (d[\"pct_neu\"]*100).round(1),\n",
        "    )\n",
        ")\n",
        "\n",
        "summary_cols = [\n",
        "    \"video_id\", \"channelTitle\", \"publishedAt\", \"viewCount\", \"likeCount\", \"commentCount\",\n",
        "    \"title_compound\", \"description_compound\", \"n_comments\", \"mean_compound\", \"pct_pos\", \"pct_neg\", \"pct_neu\", \"title\"\n",
        "]\n",
        "\n",
        "summary[summary_cols].sort_values(by=[\"mean_compound\"], ascending=False).head(10)\n"
      ],
      "id": "VHmOit66-xda"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NZNnZ0x-xdb"
      },
      "source": [
        "### 3.4 â€” Plotly Visualizations (offlineâ€‘friendly & GitHubâ€‘ready)\n",
        "Weâ€™ll create a couple of interactive charts using Plotly and save them as **selfâ€‘contained HTML** (works offline) and as **PNG** (renders in GitHub README previews).\n",
        "\n",
        "**Tip:** GitHub wonâ€™t render Plotly HTML inline in the repo view, but you can:\n",
        "- Click the HTML file and then **Raw** to open it, or\n",
        "- Serve it via **GitHub Pages**, or\n",
        "- Use the **PNG** in your README and link to the HTML for interactivity.\n"
      ],
      "id": "_NZNnZ0x-xdb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoCyys08-xdb"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# Install Plotly + Kaleido (for saving static PNGs)\n",
        "!pip -q install --upgrade plotly kaleido\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "\n",
        "# Set a renderer suitable for Colab. Alternatives: 'notebook_connected', 'svg', 'png'\n",
        "pio.renderers.default = \"colab\"\n",
        "\n",
        "# --- 1) Bar chart: Top 10 videos by mean comment sentiment (requires comments) ---\n",
        "import pandas as pd\n",
        "\n",
        "if 'summary' in globals() and not summary.empty and summary['mean_compound'].notna().any():\n",
        "    top10 = summary.sort_values(\"mean_compound\", ascending=False).head(10).copy()\n",
        "    # Truncate long titles for readability\n",
        "    top10[\"title_short\"] = top10[\"title\"].str.slice(0, 60) + top10[\"title\"].apply(lambda t: \"â€¦\" if len(str(t)) > 60 else \"\")\n",
        "\n",
        "    fig_bar = px.bar(\n",
        "        top10,\n",
        "        x=\"title_short\",\n",
        "        y=\"mean_compound\",\n",
        "        hover_data=[\"title\", \"channelTitle\", \"viewCount\", \"likeCount\", \"n_comments\"],\n",
        "        title=\"Top 10 videos by mean comment sentiment (compound)\",\n",
        "        labels={\"title_short\": \"Video title (truncated)\", \"mean_compound\": \"Mean compound sentiment\"},\n",
        "    )\n",
        "    fig_bar.update_layout(xaxis_tickangle=-30)\n",
        "    fig_bar.show()\n",
        "\n",
        "    # Save interactive HTML (self-contained) and PNG (static preview-friendly)\n",
        "    fig_bar.write_html(\"plot_top10_sentiment.html\", include_plotlyjs=\"cdn\", full_html=True)\n",
        "    fig_bar.write_image(\"plot_top10_sentiment.png\")\n",
        "\n",
        "else:\n",
        "    print(\"No comment sentiment available to plot. Make sure you fetched comments and computed 'mean_compound'.\")\n"
      ],
      "id": "DoCyys08-xdb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpW_v-3B-xdb"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# --- 2) Scatter: Relationship between viewCount and mean comment sentiment ---\n",
        "if 'summary' in globals() and not summary.empty and summary['mean_compound'].notna().any():\n",
        "    scatter_df = summary.dropna(subset=[\"mean_compound\"]).copy()\n",
        "    # Use log scale for views if counts vary widely\n",
        "    fig_scatter = px.scatter(\n",
        "        scatter_df,\n",
        "        x=\"viewCount\",\n",
        "        y=\"mean_compound\",\n",
        "        hover_name=\"title\",\n",
        "        hover_data=[\"channelTitle\", \"likeCount\", \"n_comments\"],\n",
        "        title=\"View count vs. mean comment sentiment\",\n",
        "        labels={\"viewCount\": \"Views\", \"mean_compound\": \"Mean compound sentiment\"},\n",
        "    )\n",
        "    fig_scatter.update_xaxes(type=\"log\")\n",
        "\n",
        "    fig_scatter.show()\n",
        "\n",
        "    # Save HTML + PNG\n",
        "    fig_scatter.write_html(\"plot_views_vs_sentiment.html\", include_plotlyjs=\"cdn\", full_html=True)\n",
        "    fig_scatter.write_image(\"plot_views_vs_sentiment.png\")\n",
        "else:\n",
        "    print(\"No sentiment summary to plot. Ensure the aggregation step ran successfully.\")\n"
      ],
      "id": "xpW_v-3B-xdb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0KsYfi9-xda"
      },
      "source": [
        "### 3.5 Save your datasets"
      ],
      "id": "b0KsYfi9-xda"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HolpISM-xda"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# Export tidy CSVs for later analysis or visualization\n",
        "videos_df.to_csv(\"videos_search_hits.csv\", index=False)\n",
        "video_details_df.to_csv(\"video_details.csv\", index=False)\n",
        "comments_df.to_csv(\"video_comments.csv\", index=False)\n",
        "summary.to_csv(\"video_sentiment_summary.csv\", index=False)\n",
        "\n",
        "print(\"Saved: videos_search_hits.csv, video_details.csv, video_comments.csv, video_sentiment_summary.csv\")\n"
      ],
      "id": "8HolpISM-xda"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4 â€” Calculate Sentiment Scores for a State of the Union Address\n"
      ],
      "metadata": {
        "id": "zvbbWmw3A-9V"
      },
      "id": "zvbbWmw3A-9V"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vlmoTDJ1A9Du"
      },
      "id": "vlmoTDJ1A9Du"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c064007"
      },
      "source": [
        "In this section, we are going to calculate sentiment scores for President Biden's 2023 State of the Union Address."
      ],
      "id": "7c064007"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fccaefd"
      },
      "source": [
        "First, we need use web scraping tools to collect the transcript from the 2023 State of the Union Address. This White House [URL](https://www.whitehouse.gov/briefing-room/speeches-remarks/2023/02/07/remarks-of-president-joe-biden-state-of-the-union-address-as-prepared-for-delivery/) contains the complete transcript."
      ],
      "id": "9fccaefd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33d818df"
      },
      "source": [
        "To start, we need to bring in our \"requests\" library into our Python environment and next we can make our data request using the URL:"
      ],
      "id": "33d818df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d7bbdc7"
      },
      "outputs": [],
      "source": [
        "import requests"
      ],
      "id": "0d7bbdc7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d6bbd89"
      },
      "outputs": [],
      "source": [
        "response = requests.get(\"https://www.whitehouse.gov/briefing-room/speeches-remarks/2023/02/07/remarks-of-president-joe-biden-state-of-the-union-address-as-prepared-for-delivery/\")"
      ],
      "id": "0d6bbd89"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6bd0011"
      },
      "source": [
        "Next, we can check to see whether or not the request was successful:"
      ],
      "id": "c6bd0011"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae2287dd"
      },
      "outputs": [],
      "source": [
        "response"
      ],
      "id": "ae2287dd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32adbdaf"
      },
      "source": [
        "In order to get the text data from the response we need to apply the .text method, and we can save the results in a new varibale hltm_string. The results from the data request will be in [HTML format](https://www.udacity.com/blog/2021/04/html-for-dummies.html)."
      ],
      "id": "32adbdaf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16a9955d"
      },
      "outputs": [],
      "source": [
        "html_string = response.text\n",
        "print(html_string)"
      ],
      "id": "16a9955d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dcbe20b"
      },
      "source": [
        "Let's bring in our [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) Python library to help us clean up and decode this HTML text data:"
      ],
      "id": "2dcbe20b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3ac31fa"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "id": "c3ac31fa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7e19777"
      },
      "source": [
        "Let's run our html_string variable through the Beautiful Soup object and use the get_text() function to extract the text from the HTML data. Then, let's use the print function to visualize our results:"
      ],
      "id": "d7e19777"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37c31116"
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(html_string)\n",
        "speech = soup.get_text()\n",
        "print(speech)"
      ],
      "id": "37c31116"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dc87307"
      },
      "source": [
        "Let's save our results in a text file:"
      ],
      "id": "8dc87307"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a3c5300"
      },
      "outputs": [],
      "source": [
        "with open(\"2023_union.txt\",\"w\") as file:\n",
        "    file.write(speech)"
      ],
      "id": "1a3c5300"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0f6d958"
      },
      "source": [
        "Next, let's read in the text file and also replace line breaks with spaces to because there are line breaks in the middle of sentences."
      ],
      "id": "f0f6d958"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "722cf4fd"
      },
      "outputs": [],
      "source": [
        "# Read in text file\n",
        "text = open(\"2023_union.txt\").read()\n",
        "# Replace line breaks with spaces\n",
        "text = text.replace('\\n', ' ')"
      ],
      "id": "722cf4fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd11859e"
      },
      "source": [
        "### Import NLTK\n",
        "\n",
        "Next we need to break the text into sentences.\n",
        "\n",
        "An easy way to break text into sentences, or to \"tokenize\" them into sentences, is to use [NLTK](https://www.nltk.org/), a Python library for text analysis natural language processing.\n",
        "\n",
        "Let's import nltk and download the model that will help us get sentences."
      ],
      "id": "cd11859e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3ba8f9b"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "id": "c3ba8f9b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "067d1282"
      },
      "source": [
        "To break a string into individual sentences, we can use `nltk.sent_tokenize()`"
      ],
      "id": "067d1282"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afcac959"
      },
      "outputs": [],
      "source": [
        "nltk.sent_tokenize(text)"
      ],
      "id": "afcac959"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "398812e1"
      },
      "source": [
        "To get sentence numbers for each sentence, we can use `enumerate()`."
      ],
      "id": "398812e1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fca7519d"
      },
      "outputs": [],
      "source": [
        "for number, sentence in enumerate(nltk.sent_tokenize(text)):\n",
        "    print(number, sentence)"
      ],
      "id": "fca7519d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e505e62"
      },
      "source": [
        "### Make DataFrame"
      ],
      "id": "5e505e62"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e9ba43b"
      },
      "source": [
        "For convenience, we can put all of the sentences into a pandas DataFrame. One easy way to make a DataFrame is to first make a list of dictionaries.\n",
        "\n",
        "Below we loop through the sentences, calculate sentiment scores, and then create a dictionary with the sentence, sentence number, and compound score, which we append to the list `sentence_scores`."
      ],
      "id": "0e9ba43b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e7af7ff"
      },
      "outputs": [],
      "source": [
        "# Break text into sentences\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "# Make empty list\n",
        "sentence_scores = []\n",
        "# Get each sentence and sentence number, which is what enumerate does\n",
        "for number, sentence in enumerate(sentences):\n",
        "    # Use VADER to calculate sentiment\n",
        "    scores = sentimentAnalyser.polarity_scores(sentence)\n",
        "    # Make dictionary and append it to the previously empty list\n",
        "    sentence_scores.append({'sentence': sentence, 'sentence_number': number+1, 'sentiment_score': scores['compound']})"
      ],
      "id": "1e7af7ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9865c034"
      },
      "source": [
        "To make this list of dictionaries into a DataFrame, we can simply use `pd.DataFrame()`"
      ],
      "id": "9865c034"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a44e9a28"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(sentence_scores)"
      ],
      "id": "a44e9a28"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ebba271"
      },
      "source": [
        "Let's examine the 10 most negative sentences."
      ],
      "id": "9ebba271"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c096b11"
      },
      "outputs": [],
      "source": [
        "# Assign DataFrame to variable red_df\n",
        "speech_df = pd.DataFrame(sentence_scores)\n",
        "\n",
        "# Sort by the column \"sentiment_score\" and slice for first 10 values\n",
        "speech_df.sort_values(by='sentiment_score')[:10]"
      ],
      "id": "9c096b11"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be6c20a8"
      },
      "source": [
        "Let's examine the 10 most positive sentences."
      ],
      "id": "be6c20a8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5da6adc"
      },
      "outputs": [],
      "source": [
        "# Sort by the column \"sentiment_score,\" this time in descending order, and slice for first 10 values\n",
        "speech_df.sort_values(by='sentiment_score', ascending=False)[:10]"
      ],
      "id": "d5da6adc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "012fecf1"
      },
      "source": [
        "### Make a Sentiment Plot"
      ],
      "id": "012fecf1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a0cfe52"
      },
      "source": [
        "To create a data visualization of sentiment over the course of the 2023 State of the Union Address we can plot the sentiment scores over story time (aka sentence number)."
      ],
      "id": "3a0cfe52"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "274038af"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px"
      ],
      "id": "274038af"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a522c275"
      },
      "outputs": [],
      "source": [
        "fig = px.line(speech_df, x='sentence_number', y=\"sentiment_score\",\n",
        "             title= \"Sentiment Analysis of 2023 State of the Union Address\")\n",
        "fig.show()"
      ],
      "id": "a522c275"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92f6bd35"
      },
      "source": [
        "We can also get a more generalized view by getting a \"rolling average\" 5 sentences at a time by using the `.rolling()` method with a specified window and storing the results in a new column \"speech_roll\":"
      ],
      "id": "92f6bd35"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d0453b6"
      },
      "outputs": [],
      "source": [
        "speech_df['speech_roll'] = speech_df.rolling(5)['sentiment_score'].mean()"
      ],
      "id": "3d0453b6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c815c586"
      },
      "outputs": [],
      "source": [
        "speech_df[:25]"
      ],
      "id": "c815c586"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51660730"
      },
      "outputs": [],
      "source": [
        "fig = px.line(speech_df, x='sentence_number', y=\"speech_roll\",\n",
        "             title= \"Sentiment Analysis of 2023 State of the Union Address\")\n",
        "fig.show()"
      ],
      "id": "51660730"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBH9X14M-xda"
      },
      "source": [
        "## Part 5 â€” Student Work: Improve & Explore (Prompts you can use with a GenAI model)\n",
        "\n",
        "## ðŸŽ¬ What the \"Student-Driven\" Hour Is About\n",
        "During the final hour of class, youâ€™ll take the working pipeline we built together and:\n",
        "- choose your own topic (e.g., climate news, celebrity content, policy debates)\n",
        "- collect a small YouTube dataset (videos + comments)\n",
        "- run VADER sentiment\n",
        "- **improve one part of the script** using a GenAI assistant (pagination, cleaning, retry logic, parameters, etc.)\n",
        "- export your results & write a short insight summary\n",
        "\n",
        "**Your goal isnâ€™t to build the perfect tool â€” itâ€™s to experiment, debug, and deepen your understanding.**\n",
        "\n",
        "Success = a working query, a sentiment summary, one improvement to your code, and 3â€“5 bullet insights.\n",
        "\n",
        "---\n",
        "Copy/paste any of these and adapt to your notebook:\n",
        "\n",
        "1) **Pagination helper**  \n",
        "*â€œGiven my YouTube comments code, write a loop that continues until `nextPageToken` is missing or I reach a cap (e.g., 500 comments). Add docstrings and basic error handling.â€*\n",
        "\n",
        "2) **Retry + backoff**  \n",
        "*â€œAdd retries with exponential backoff for HTTP 5xx and a polite sleep for 403/429. Keep dependencies minimal.â€*\n",
        "\n",
        "3) **Parameters refactor**  \n",
        "*â€œRefactor my API calls so params live in wellâ€‘named dicts with comments for each parameter.â€*\n",
        "\n",
        "4) **CSV schema & types**  \n",
        "*â€œReview my DataFrame dtypes for `videos` and `comments` and cast to sensible types (`Int64`, `datetime64[ns]`). Update the save step.â€*\n",
        "\n",
        "5) **Text cleaning utility**  \n",
        "*â€œWrite `clean_text(s)` that removes URLs, collapses whitespace, strips markup, and optionally normalizes emojis.â€*\n",
        "\n",
        "6) **Perâ€‘video rollup**  \n",
        "*â€œGiven a comments DataFrame with `video_id` + `compound`, return n, mean, std, and pos/neg/neu proportions per video using VADER thresholds.â€*\n",
        "\n",
        "7) **Visualization**  \n",
        "*â€œPlot the relationship between `viewCount` and `mean comment sentiment`, labeling outliers by a truncated title.â€*\n",
        "\n",
        "8) **Time window**  \n",
        "*â€œModify `search.list` to include `publishedAfter` / `publishedBefore` (RFC3339) and pass them into the workflow.â€*\n",
        "\n",
        "9) **Topic comparison**  \n",
        "*â€œCompare two queries (A vs B) and test whether mean comment sentiment differs (Mannâ€‘Whitney U).â€*\n",
        "\n",
        "10) **Language handling**  \n",
        "*â€œDetect nonâ€‘English comments (e.g., `langdetect`) and either filter or tag them; update aggregation to show language mix.â€*"
      ],
      "id": "NBH9X14M-xda"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gb2Z-8y-xda"
      },
      "source": [
        "## Appendix A â€” How VADER's *compound* score is computed (and why it's favored)\n",
        "**High level:** VADER uses a humanâ€‘curated lexicon of words with positive/negative \"valence\" scores and a set of rules for emphasis, negation, punctuation, capitalization, and emojis. It computes four scores: `pos`, `neu`, `neg` (proportions) and `compound` (a single summary in [-1, 1]).\n",
        "\n",
        "**Steps (simplified):**\n",
        "1. **Tokenize** the text and look up each tokenâ€™s **valence** in the VADER lexicon.\n",
        "2. Apply **rules** and **modifiers**:\n",
        "   - **Booster/intensifier words** (e.g., *very, extremely*) scale valence.\n",
        "   - **Negation** (e.g., *not, never*) flips or reduces polarity.\n",
        "   - **Punctuation & capitalization** (e.g., `!!!`, ALLâ€‘CAPS) amplify valence.\n",
        "   - **Emojis/emoticons** also carry polarity.\n",
        "3. **Sum** the adjusted valences across the text â†’ call this `S`.\n",
        "4. **Normalize** to [-1, 1] using:\n",
        "\n",
        "   \\[ \\textbf{compound} = \\frac{S}{\\sqrt{S^2 + \\alpha}} \\quad \\text{with } \\alpha = 15 \\]\n",
        "\n",
        "**Why people favor `compound`:**\n",
        "- Itâ€™s a **single number** summarizing overall sentiment (great for ranking, correlations, and plots).\n",
        "- The normalization makes scores **comparable** across different sentence lengths and emphasis.\n",
        "- Itâ€™s easy to threshold: commonly, `> 0.05` â†’ positive, `< -0.05` â†’ negative, otherwise neutral.\n",
        "\n",
        "**Caveats:** sarcasm, domainâ€‘specific slang, and mixed statements can still confuse any lexiconâ€‘based method.\n"
      ],
      "id": "4gb2Z-8y-xda"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}